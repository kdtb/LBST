{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d8b89a-750f-4c30-b4eb-119ff4070c05",
   "metadata": {},
   "source": [
    "# Data preparation (customDataset.py)\n",
    "\n",
    "https://www.learnpytorch.io/04_pytorch_custom_datasets/#what-is-a-custom-dataset\n",
    "https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb\n",
    "\n",
    "Build custom dataset\n",
    "\n",
    "- Organize into folders\n",
    "- Turn into tensors\n",
    "\n",
    "Image classification format contains separate classes of images in seperate directories titled with a particular class name.\n",
    "\n",
    "The format we aim for:\n",
    "\n",
    "LBST/ <- overall dataset folder\n",
    "    train/ <- training images\n",
    "        Approved/ <- class name as folder name\n",
    "            Parcel-1\n",
    "                image01.jpeg\n",
    "                image02.jpeg\n",
    "                ...\n",
    "            Parcel-2\n",
    "                image-05.jpeg\n",
    "                image-06.jpeg\n",
    "                ...\n",
    "        Not-Approved/\n",
    "            Parcel-3\n",
    "                image07.jpeg\n",
    "                image08.jpeg\n",
    "                ...\n",
    "            Parcel-4\n",
    "                image-09.jpeg\n",
    "                image-10.jpeg\n",
    "                ...\n",
    "    test/ <- testing images\n",
    "        Approved/ <- class name as folder name\n",
    "            Parcel-5\n",
    "                image11.jpeg\n",
    "                image12.jpeg\n",
    "                ...\n",
    "            Parcel-6\n",
    "                image-13.jpeg\n",
    "                image-14.jpeg\n",
    "                ...\n",
    "        Not-Approved/\n",
    "            Parcel-7\n",
    "                image15.jpeg\n",
    "                image16.jpeg\n",
    "                ...\n",
    "            Parcel-8\n",
    "                image-17.jpeg\n",
    "                image-18.jpeg\n",
    "                ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dffa8aa4-e782-4cfa-9810-844b331a29eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x90 in position 413: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(i):\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(i, filename)) \u001b[38;5;28;01mas\u001b[39;00m filedata:\n\u001b[1;32m----> 8\u001b[0m             string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mfiledata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msplit())\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Open a file\u001b[39;00m\n\u001b[0;32m     12\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/Approved\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\.conda\\envs\\LBST\\lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcharmap_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdecoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x90 in position 413: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, sys\n",
    "\n",
    "\n",
    "path = [\"C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/Approved\", \"C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/NonApproved\"]\n",
    "for i in path:\n",
    "    for filename in os.listdir(i):\n",
    "        with open(os.path.join(i, filename), 'r') as filedata:\n",
    "            string = \"\".join(filedata.read().split())\n",
    "\n",
    "\n",
    "# Open a file\n",
    "path = \"C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/Approved\"\n",
    "dirs = os.listdir( path )\n",
    "\n",
    "# This would print all the files and directories\n",
    "name2label = {}\n",
    "for file in dirs:\n",
    "   print(file)\n",
    "   name2label[file] = 0#len(name2label.keys())\n",
    "\n",
    "name2label\n",
    "\n",
    "\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e4d76e6-5eaf-429a-b42d-4973353b0672",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (386149845.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[52], line 8\u001b[1;36m\u001b[0m\n\u001b[1;33m    path = f'C:\\Users\\kaspe\\OneDrive - Aarhus Universitet\\Skrivebord\\BI\\4. semester\\Data\\LBST\\Danish Challenge\\2023 J#\\Approved'\u001b[0m\n\u001b[1;37m                                                                                                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing: make .csv file containing file-path (xxx.jpeg), parcel-ID, and class\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "# Open a file\n",
    "path = f'C:\\Users\\kaspe\\OneDrive - Aarhus Universitet\\Skrivebord\\BI\\4. semester\\Data\\LBST\\Danish Challenge\\2023 J#\\Approved'\n",
    "dirs = os.listdir( path )\n",
    "\n",
    "# This would print all the files and directories\n",
    "for file in dirs:\n",
    "   print(file)\n",
    "\n",
    "\n",
    "#load_csv(r'C:\\Users\\kaspe\\OneDrive - Aarhus Universitet\\Skrivebord\\BI\\4. semester\\Data\\LBST\\Danish Challenge\\2023 J#', label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aefddb9f-a164-4d52-9d8f-289feeb97fa9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LBSTDataset' from 'customDataset' (C:\\Users\\kaspe\\LBST\\customDataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcustomDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LBSTDataset\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m dataset \u001b[38;5;241m=\u001b[39m LBSTDataset(csv_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, root_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     16\u001b[0m                       transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mToTensor())\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LBSTDataset' from 'customDataset' (C:\\Users\\kaspe\\LBST\\customDataset.py)"
     ]
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=ZoZHd0Zm3RY&ab_channel=AladdinPersson\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from customDataset import LBSTDataset\n",
    "\n",
    "# Load data: From images to tensor\n",
    "\n",
    "dataset = LBSTDataset(csv_file = '', root_dir = '',\n",
    "                      transform = transforms.ToTensor())\n",
    "\n",
    "# Split data properly without information leak\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [20000,5000])\n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Model\n",
    "\n",
    "# Rest of code found: https://www.youtube.com/watch?v=ZoZHd0Zm3RY&ab_channel=AladdinPersson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c6f5b-bba8-40f8-9fb7-74477a9ae88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d272e-9538-45dc-8119-3ae440978ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc61b1-209c-486f-b879-420fad403557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e71e86-1e0a-4c7d-bbaf-6d05c6789f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68726414-4cd2-4875-80b5-79bd34d17e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f917a7-2147-4233-a84e-552cf66655b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4cfbc8-cc06-4a0e-90e6-f0d0f904924e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4db5f-347e-4292-93f8-2d1b22b7afbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66518f-e1f3-4167-896f-e9a566e68684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7300595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92fedcb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kaspe'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f00b6d",
   "metadata": {},
   "source": [
    "# data_loader.py\n",
    "specifies how the data should be fed to the network: LightningDataModule\n",
    "\n",
    "In short, data preparation has 4 steps:\n",
    "\n",
    "- Download images\n",
    "- Image transforms (these are highly subjective).\n",
    "- Generate training, validation and test dataset splits.\n",
    "- Wrap each dataset split in a DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fbf82a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lightning.ai/docs/pytorch/latest/data/datamodule.html\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Note - you must have torchvision installed for this example\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./\"):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\":\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\":\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=32)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=32)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=32)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.mnist_predict, batch_size=32)\n",
    "\n",
    "data_module = MNISTDataModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb11905",
   "metadata": {},
   "source": [
    "# model.py\n",
    "specifies the neural network architecture, the loss function and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a969db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "class LightningMNISTClassifier(pl.LightningModule):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # mnist images are (1, 28, 28) (channels, width, height) \n",
    "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
    "    self.layer_2 = torch.nn.Linear(128, 256)\n",
    "    self.layer_3 = torch.nn.Linear(256, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "      batch_size, channels, width, height = x.size()\n",
    "\n",
    "      # (b, 1, 28, 28) -> (b, 1*28*28)\n",
    "      x = x.view(batch_size, -1)\n",
    "\n",
    "      # layer 1 (b, 1*28*28) -> (b, 128)\n",
    "      x = self.layer_1(x)\n",
    "      x = torch.relu(x)\n",
    "\n",
    "      # layer 2 (b, 128) -> (b, 256)\n",
    "      x = self.layer_2(x)\n",
    "      x = torch.relu(x)\n",
    "\n",
    "      # layer 3 (b, 256) -> (b, 10)\n",
    "      x = self.layer_3(x)\n",
    "\n",
    "      # probability distribution over labels\n",
    "      x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "      return x\n",
    "\n",
    "  def cross_entropy_loss(self, logits, labels):\n",
    "    return F.nll_loss(logits, labels)\n",
    "\n",
    "  def training_step(self, train_batch, batch_idx):\n",
    "      x, y = train_batch\n",
    "      logits = self.forward(x)\n",
    "      loss = self.cross_entropy_loss(logits, y)\n",
    "      self.log('train_loss', loss)\n",
    "      return loss\n",
    "\n",
    "\n",
    "  def validation_step(self, val_batch, batch_idx):\n",
    "      x, y = val_batch\n",
    "      logits = self.forward(x)\n",
    "      loss = self.cross_entropy_loss(logits, y)\n",
    "      self.log('val_loss', loss)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "model = LightningMNISTClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0b9be",
   "metadata": {},
   "source": [
    "# train.py\n",
    "train models and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134017a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Save the model with help from utils.py\n",
    "utils.save_model(model=model1,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"Scratch.pth\")\n",
    "utils.save_model(model=model2,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"ResNet-18.pth\")\n",
    "utils.save_model(model=model3,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"VGG-16.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59998050",
   "metadata": {},
   "source": [
    "# evaluate.py\n",
    "contains the main loop for evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1cb63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d731e7f",
   "metadata": {},
   "source": [
    "# search_hyperparams.py\n",
    "hyper parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96388e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed44f43e",
   "metadata": {},
   "source": [
    "# synthesize_results.py\n",
    "An author synthesizes study data by combining the results together to enable comparison and to allow others to draw further conclusions from them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba26f56",
   "metadata": {},
   "source": [
    "# evaluate.py\n",
    "contains the main loop for evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b82219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c28a1e5",
   "metadata": {},
   "source": [
    "# utils.py\n",
    "utility functions for handling hyperparams/logging/storing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81012979",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ce356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
