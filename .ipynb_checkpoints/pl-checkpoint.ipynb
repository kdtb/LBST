{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "50907ddb-57fc-421d-b038-660803f6c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "def set_all_seeds(seed):\n",
    "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "74d63d06-b4b0-4d81-b106-0e07efd742a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device cuda for GPU if it's available, otherwise run on the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 3 * 224 * 224\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "random_seed = 1\n",
    "set_all_seeds(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "babc847c-e88f-4b58-999d-332c4c5023a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce code length by creating a _common_step function, in order not to have to repeat code in training_step, validation_step and test_step\n",
    "\n",
    "\n",
    "class NN(pl.LightningModule): # pl.LightningModule inherits from nn.Module and adds extra functionality\n",
    "    def __init__(self, input_size, num_classes): # In the constructor, you declare all the layers you want to use.\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x): # Forward function computes output Tensors from input Tensors. In the forward function, you define how your model is going to be run, from input to output. We're accepting only a single input in here, but if you want, feel free to use more\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        loss = self.loss_fn(scores, y)\n",
    "        return loss, scores, y\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1) # flattening\n",
    "        scores = self.forward(x)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9a429a32-b9b2-4772-96d5-9b581a927692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'Approved', 'NonApproved', 'test_set.csv', 'train_set.csv', 'val_set.csv', 'xlbst.csv']\n"
     ]
    }
   ],
   "source": [
    "# Find folder paths\n",
    "\n",
    "base_path = r\"C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/\"\n",
    "target_dirs = os.listdir(base_path)\n",
    "print(target_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a4ce751e-64e0-4586-b91f-fd9c9af60343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 separate df's: one for Approved images, one for NonApproved, containing file_name and label\n",
    "\n",
    "# Assign label = 0 to Approved images\n",
    "approved = pd.DataFrame(data = os.listdir(os.path.join(base_path, target_dirs[1])), columns = ['file_name'])\n",
    "approved = approved.assign(label = 0)\n",
    "\n",
    "# Assign label = 1 to NonApproved images\n",
    "nonapproved = pd.DataFrame(data = os.listdir(os.path.join(base_path, target_dirs[2])), columns = ['file_name'])\n",
    "nonapproved = nonapproved.assign(label = 1)\n",
    "\n",
    "# Merge into 1 df\n",
    "df = pd.concat([approved, nonapproved])\n",
    "\n",
    "# Add parcel_id column containing character 3-10 from file_name column\n",
    "df['parcel_id'] = df['file_name'].str[3:10]\n",
    "\n",
    "# Write .csv\n",
    "df.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/xlbst.csv', sep = ',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6e50f125-25c0-4b2c-bd7f-454dcefe8bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name  label parcel_id\n",
      "0   22-0223605_F78EFF88702EA742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "1   22-0223605_F78EFF887030A742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "2   22-0223605_F78EFF88703AA742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "28  22-0225160_F78EFF887012A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "29  22-0225160_F78EFF887013A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "30  22-0225160_F78EFF887014A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "31  22-0225160_F78EFF887015A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "32  22-0225160_F78EFF887016A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "33  22-0225160_F78EFF887017A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "34  22-0225160_F78EFF88701AA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "35  22-0225160_F78EFF88701BA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "36  22-0225160_F78EFF88701CA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "37  22-0225160_F78EFF88701DA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "38  22-0225160_F78EFF88701EA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "39  22-0225160_F78EFF88701FA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "Test set length:\n",
      "15\n",
      "Train set length:\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Split group by\n",
    "\n",
    "set_all_seeds(random_seed)\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.20, n_splits=2, random_state=random_seed)\n",
    "split = splitter.split(df, groups=df.parcel_id)\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_set = df.iloc[train_inds]\n",
    "test_set = df.iloc[test_inds]\n",
    "\n",
    "\n",
    "print(test_set.to_string())\n",
    "print('Test set length:',len(test_set), 'Train set length:', len(train_set), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "5ef860cb-52a0-4d22-a6c9-68e95b955b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into 80/20 train/val\n",
    "\n",
    "train_set2 = train_set\n",
    "splitter2 = GroupShuffleSplit(test_size=0.2, n_splits =1, random_state=random_seed)\n",
    "split2 = splitter2.split(train_set2, groups=train_set2.parcel_id)\n",
    "train_inds2, val_inds = next(split2)\n",
    "\n",
    "train_set2 = train_set.iloc[train_inds2]\n",
    "val_set = train_set.iloc[val_inds]\n",
    "\n",
    "# Print train, val, test\n",
    "#print(train_set.to_string())\n",
    "#print(train_set2)\n",
    "#print(val_set)\n",
    "#print(len(train_set), len(train_set2), len(val_set))\n",
    "\n",
    "# Save to csv\n",
    "\n",
    "train_set2.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/train_set.csv', sep = ',', encoding='utf-8', index=False)\n",
    "val_set.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/val_set.csv', sep = ',', encoding='utf-8', index=False)\n",
    "test_set.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/test_set.csv', sep = ',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4bbbaf77-cf65-41a9-a3f5-8cc1b4ebccbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<customDataset.LBSTDataset object at 0x000001BB00118880>\n",
      "The shape of tensor for 50th image in train dataset:  torch.Size([3, 224, 224])\n",
      "The label for 50th image in train dataset:  tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=ZoZHd0Zm3RY&ab_channel=AladdinPersson\n",
    "\n",
    "set_all_seeds(random_seed)\n",
    "\n",
    "# Create PyTorch compatible datasets: From images to tensors\n",
    "\n",
    "from customDataset import LBSTDataset\n",
    "train_set = LBSTDataset(csv_file = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/train_set.csv', \n",
    "                      root_dir = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/All',\n",
    "                      transform = transforms.Compose([\n",
    "                                                transforms.Resize((224, 224)),\n",
    "                                                #transforms.RandomHorizontalFlip(),\n",
    "                                                #transforms.RandomResizedCrop(224),\n",
    "                                                transforms.ToTensor()#,\n",
    "                                                #transforms.Normalize()\n",
    "                      ]))\n",
    "\n",
    "val_set = LBSTDataset(csv_file = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/val_set.csv', \n",
    "                      root_dir = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/All',\n",
    "                      transform = transforms.Compose([\n",
    "                                                transforms.Resize((224, 224)),\n",
    "                                                #transforms.RandomHorizontalFlip(),\n",
    "                                                #transforms.RandomResizedCrop(224),\n",
    "                                                transforms.ToTensor()#,\n",
    "                                                #transforms.Normalize()\n",
    "                      ]))\n",
    "\n",
    "test_set = LBSTDataset(csv_file = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/test_set.csv', \n",
    "                      root_dir = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/All',\n",
    "                      transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "\n",
    "len(train_set)\n",
    "print(train_set)\n",
    "print('The shape of tensor for 50th image in train dataset: ',train_set[49][0].shape)\n",
    "print('The label for 50th image in train dataset: ',train_set[49][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c205fe32-ad98-45ad-b0de-72f510ddc868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print 1st batch of image tensor: torch.Size([32, 3, 224, 224])\n",
      "print batch of corresponding labels: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "set_all_seeds(random_seed)\n",
    "\n",
    "# Train, val and test loader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True) # Shuffling is done during the training to make sure we aren’t exposing our model to the same cycle (order) of data in every epoch. \n",
    "\n",
    "val_loader = DataLoader(dataset=val_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False) # You don’t need to shuffle the validation and test datasets, since no training is done, the model is used in model.eval() and thus the order of samples won’t change the results.\n",
    "\n",
    "test_loader = DataLoader(dataset=test_set,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)\n",
    "\n",
    "# print batch of image tensor\n",
    "print('print 1st batch of image tensor:', next(iter(train_loader))[0].shape)\n",
    "print('print batch of corresponding labels:', next(iter(train_loader))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "306a1bb2-37ff-40ce-baac-44c053be5cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(input_size=input_size, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8cc6bf1e-e4a1-4bdf-b131-0fddf0c71f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ef4b4b22-c872-4033-b5e0-5d6d73ff9786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type             | Params\n",
      "---------------------------------------------\n",
      "0 | fc1     | Linear           | 7.5 M \n",
      "1 | fc2     | Linear           | 102   \n",
      "2 | loss_fn | CrossEntropyLoss | 0     \n",
      "---------------------------------------------\n",
      "7.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.5 M     Total params\n",
      "30.106    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaspe\\.conda\\envs\\LBST\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaspe\\.conda\\envs\\LBST\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\kaspe\\.conda\\envs\\LBST\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:01<00:00,  1.43it/s, v_num=11]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s, v_num=11]33it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:01<00:00,  1.46it/s, v_num=11]       \u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:01<00:00,  1.13it/s, v_num=11]08it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:01<00:00,  1.48it/s, v_num=11]       \u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:01<00:00,  1.16it/s, v_num=11]51it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:01<00:00,  1.15it/s, v_num=11]       \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2/2 [00:01<00:00,  1.02it/s, v_num=11]\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 98.89it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   1.662491740717087e-05   </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  1.662491740717087e-05  \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaspe\\.conda\\envs\\LBST\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:430: PossibleUserWarning: The dataloader, test_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 111.38it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">   4.881039058091119e-05   </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m  4.881039058091119e-05  \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 4.881039058091119e-05}]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train network\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "\n",
    "seed_everything(42, workers=True) # By setting workers=True in seed_everything(), Lightning derives unique seeds across all dataloader workers and processes for torch, numpy and stdlib random number generators. When turned on, it ensures that e.g. data augmentations are not repeated across workers.\n",
    "\n",
    "trainer = pl.Trainer(accelerator='auto', min_epochs=1, max_epochs=3, deterministic=True) # deterministic ensures random seed reproducibility\n",
    "# trainer.tune finds optimal hyper parameters, eg batch size and learning rate\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.validate(model, val_loader)\n",
    "trainer.test(model, test_loader)\n",
    "\n",
    "# A general place to start is to set num_workers equal to the number of CPU cores on that machine. You can get the number of CPU cores in python using os.cpu_count(), but note that depending on your batch size, you may overflow RAM memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f481a-a06e-4d6d-9831-4ab5c9a3202f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852e048-f989-4adf-b8a9-063ed37a8dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc029c-c7b7-4f55-b840-68af44817f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e022e8a-891a-4cb6-b525-aba88d04f8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ac6c5b4c-43cf-42c9-94f2-ad8c1488411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class NN(pl.LightningModule): # pl.LightningModule inherits from nn.Module and adds extra functionality\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x) # compute scores\n",
    "        loss = F.loss_fn(scores, y) # compute losses\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x) # compute scores\n",
    "        loss = F.loss_fn(scores, y) # compute losses\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x) # compute scores\n",
    "        loss = F.loss_fn(scores, y) # compute losses\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
