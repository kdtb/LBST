{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d8b89a-750f-4c30-b4eb-119ff4070c05",
   "metadata": {},
   "source": [
    "# Data preparation (customDataset.py)\n",
    "\n",
    "https://www.learnpytorch.io/04_pytorch_custom_datasets/#what-is-a-custom-dataset\n",
    "https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb\n",
    "\n",
    "Build custom dataset\n",
    "\n",
    "- Organize into folders\n",
    "- Turn into tensors\n",
    "\n",
    "Image classification format contains separate classes of images in seperate directories titled with a particular class name.\n",
    "\n",
    "The format we aim for:\n",
    "\n",
    "LBST/ <- overall dataset folder\n",
    "    train/ <- training images\n",
    "        Approved/ <- class name as folder name\n",
    "            Parcel-1\n",
    "                image01.jpeg\n",
    "                image02.jpeg\n",
    "                ...\n",
    "            Parcel-2\n",
    "                image-05.jpeg\n",
    "                image-06.jpeg\n",
    "                ...\n",
    "        Not-Approved/\n",
    "            Parcel-3\n",
    "                image07.jpeg\n",
    "                image08.jpeg\n",
    "                ...\n",
    "            Parcel-4\n",
    "                image-09.jpeg\n",
    "                image-10.jpeg\n",
    "                ...\n",
    "    test/ <- testing images\n",
    "        Approved/ <- class name as folder name\n",
    "            Parcel-5\n",
    "                image11.jpeg\n",
    "                image12.jpeg\n",
    "                ...\n",
    "            Parcel-6\n",
    "                image-13.jpeg\n",
    "                image-14.jpeg\n",
    "                ...\n",
    "        Not-Approved/\n",
    "            Parcel-7\n",
    "                image15.jpeg\n",
    "                image16.jpeg\n",
    "                ...\n",
    "            Parcel-8\n",
    "                image-17.jpeg\n",
    "                image-18.jpeg\n",
    "                ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429fba8d-034c-4f37-b580-0b1699bfdc15",
   "metadata": {},
   "source": [
    "## Create csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "dffa8aa4-e782-4cfa-9810-844b331a29eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Approved', 'NonApproved']\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Find folder paths\n",
    "\n",
    "base_path = r\"C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/\"\n",
    "target_dirs = os.listdir(base_path)\n",
    "print(target_dirs)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "722ceadd-f8f8-4a72-940c-c193cbce987c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name  label parcel_id\n",
      "0   22-0223360_F796528C3F8CECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "1   22-0223360_F796528C3F8DECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "2   22-0223360_F796528C3F8EECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "3   22-0223360_F796528C40F1ECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "4   22-0223360_F796528C40F2ECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "..                                               ...    ...       ...\n",
      "35  22-0225160_F78EFF88701BA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "36  22-0225160_F78EFF88701CA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "37  22-0225160_F78EFF88701DA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "38  22-0225160_F78EFF88701EA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "39  22-0225160_F78EFF88701FA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "\n",
      "[80 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create 2 separate df's: one for Approved images, one for NonApproved, containing file_name and label\n",
    "\n",
    "# Assign label = 0 to Approved images\n",
    "approved = pd.DataFrame(data = os.listdir(os.path.join(base_path, target_dirs[0])), columns = ['file_name'])\n",
    "approved = approved.assign(label = 0)\n",
    "\n",
    "# Assign label = 1 to NonApproved images\n",
    "nonapproved = pd.DataFrame(data = os.listdir(os.path.join(base_path, target_dirs[1])), columns = ['file_name'])\n",
    "nonapproved = nonapproved.assign(label = 1)\n",
    "\n",
    "# Merge into 1 df\n",
    "df = pd.concat([approved, nonapproved])\n",
    "\n",
    "# Add parcel_id column containing character 3-10 from file_name column\n",
    "df['parcel_id'] = df['file_name'].str[3:10]\n",
    "print(df)\n",
    "\n",
    "# Write .csv\n",
    "df.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/xlbst.csv', sep = ',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d25fe1-7635-4558-b8f7-4ca90aa803f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize images\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c285f077-db80-4f91-8e0d-1bd59f68ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.assign(parcel_id = np.nan)\n",
    "#df.to_excel(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/Courses.xlsx')\n",
    "\n",
    "#path = [\"C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/Approved\", \"C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/NonApproved\"]\n",
    "#for i in path:\n",
    "#    for filename in os.listdir(i):\n",
    "#        with open(os.path.join(i, filename), 'r') as filedata:\n",
    "#            string = \"\".join(filedata.read().split())\n",
    "\n",
    "\n",
    "#dataframe = pd.DataFrame(columns = ['file_name', 'parcel_id', 'class'])\n",
    "#dataframe.info()\n",
    "\n",
    "# Open a file\n",
    "\n",
    "#df[['class'], ['parcel_id']] = np.nan\n",
    "\n",
    "\n",
    "#for i, j in (range(Approved), range(NonApproved)):\n",
    "    \n",
    "    \n",
    "#files1.extend(files2)\n",
    "#print(df)\n",
    "\n",
    "#dataframe[0][0] = files1[0]\n",
    "\n",
    "#len(df)\n",
    "\n",
    "\n",
    "\n",
    "#new_path = []\n",
    "#for i in base_dirs:\n",
    "#    new_path[i] = os.path.join(base_path, base_dirs[i])\n",
    "\n",
    "\n",
    "#print(new_path)\n",
    "\n",
    "#for i in base_dirs:    \n",
    "#    print(os.path.join(base_path, i))\n",
    "\n",
    "    \n",
    "    #dirs = os.listdir(path)\n",
    "#    name1label = {}\n",
    "#    for file in dirs:\n",
    "#       print(file)\n",
    "#       name1label[file] = 0 #len(name2label.keys())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#path = \"C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/Approved\"\n",
    "#dirs = os.listdir( path )\n",
    "\n",
    "# This would print all the files and directories\n",
    "#name2label = {}\n",
    "#for file in dirs:\n",
    "#   print(file)\n",
    "#   name2label[file] = 0 #len(name2label.keys())\n",
    "\n",
    "#name2label\n",
    "\n",
    "\n",
    "#for key, value in inputdict.items():\n",
    "    # do something with value\n",
    "#    inputdict[key] = newvalue\n",
    "\n",
    "\n",
    "# Data preprocessing: make .csv file containing file-path (xxx.jpeg), parcel-ID, and class\n",
    "\n",
    "\n",
    "#import os\n",
    "\n",
    "\n",
    "# Open a file\n",
    "#path = f'C:\\Users\\kaspe\\OneDrive - Aarhus Universitet\\Skrivebord\\BI\\4. semester\\Data\\LBST\\Danish Challenge\\2023 J#\\Approved'\n",
    "#dirs = os.listdir( path )\n",
    "\n",
    "# This would print all the files and directories\n",
    "#for file in dirs:\n",
    "#   print(file)\n",
    "\n",
    "\n",
    "#load_csv(r'C:\\Users\\kaspe\\OneDrive - Aarhus Universitet\\Skrivebord\\BI\\4. semester\\Data\\LBST\\Danish Challenge\\2023 J#', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bdaf04-6c57-48d1-83bc-0414dfd5098f",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "aefddb9f-a164-4d52-9d8f-289feeb97fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<customDataset.LBSTDataset object at 0x000001FB52213790>\n",
      "<torch.utils.data.dataset.Subset object at 0x000001FB52212B30> <torch.utils.data.dataset.Subset object at 0x000001FB522130D0>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Unexpected type <class 'numpy.ndarray'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[177], line 58\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m     56\u001b[0m     losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     59\u001b[0m         \u001b[38;5;66;03m# Get data to cuda if possible\u001b[39;00m\n\u001b[0;32m     60\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     61\u001b[0m         targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[1;32m~\\.conda\\envs\\LBST\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\.conda\\envs\\LBST\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\.conda\\envs\\LBST\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\.conda\\envs\\LBST\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\.conda\\envs\\LBST\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\LBST\\customDataset.py:24\u001b[0m, in \u001b[0;36mLBSTDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     21\u001b[0m y_label \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mannotations\u001b[38;5;241m.\u001b[39miloc[index, \u001b[38;5;241m1\u001b[39m])) \u001b[38;5;66;03m# y_label is placed in the second column of the csv file\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 24\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# optional: if we send in trasnforms, it will transform images\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (image, y_label)\n",
      "File \u001b[1;32m~\\.conda\\envs\\LBST\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\.conda\\envs\\LBST\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\.conda\\envs\\LBST\\lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 361\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\LBST\\lib\\site-packages\\torchvision\\transforms\\functional.py:476\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    471\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    472\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    473\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    474\u001b[0m         )\n\u001b[1;32m--> 476\u001b[0m _, image_height, image_width \u001b[38;5;241m=\u001b[39m \u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m    478\u001b[0m     size \u001b[38;5;241m=\u001b[39m [size]\n",
      "File \u001b[1;32m~\\.conda\\envs\\LBST\\lib\\site-packages\\torchvision\\transforms\\functional.py:78\u001b[0m, in \u001b[0;36mget_dimensions\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mget_dimensions(img)\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dimensions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\LBST\\lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:31\u001b[0m, in \u001b[0;36mget_dimensions\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m     29\u001b[0m     width, height \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39msize\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [channels, height, width]\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Unexpected type <class 'numpy.ndarray'>"
     ]
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=ZoZHd0Zm3RY&ab_channel=AladdinPersson\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "in_channel = 3\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "\n",
    "from customDataset import LBSTDataset\n",
    "\n",
    "\n",
    "\n",
    "# Load data: From images to tensor\n",
    "\n",
    "dataset = LBSTDataset(csv_file = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/xlbst.csv', \n",
    "                      root_dir = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/All',\n",
    "                      transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "\n",
    "print(dataset)\n",
    "# Split data properly without information leak\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [60,20])\n",
    "print(train_set, test_set)\n",
    "train_loader = DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "# Model\n",
    "\n",
    "# Rest of code found: https://www.youtube.com/watch?v=ZoZHd0Zm3RY&ab_channel=AladdinPersson\n",
    "\n",
    "model = torchvision.models.googlenet(weights='DEFAULT')\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# Train network\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        #forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Cost at epoch {epoch} is {sum(losses)/len(losses)}')\n",
    "\n",
    "# Check acc on training\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100}')\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "print(\"Checking accuracy on training set\")\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print(\"Checking accuracy on test set\")\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c6f5b-bba8-40f8-9fb7-74477a9ae88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d272e-9538-45dc-8119-3ae440978ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc61b1-209c-486f-b879-420fad403557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e71e86-1e0a-4c7d-bbaf-6d05c6789f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68726414-4cd2-4875-80b5-79bd34d17e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f917a7-2147-4233-a84e-552cf66655b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4cfbc8-cc06-4a0e-90e6-f0d0f904924e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4db5f-347e-4292-93f8-2d1b22b7afbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66518f-e1f3-4167-896f-e9a566e68684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7300595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92fedcb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kaspe'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f00b6d",
   "metadata": {},
   "source": [
    "# data_loader.py\n",
    "specifies how the data should be fed to the network: LightningDataModule\n",
    "\n",
    "In short, data preparation has 4 steps:\n",
    "\n",
    "- Download images\n",
    "- Image transforms (these are highly subjective).\n",
    "- Generate training, validation and test dataset splits.\n",
    "- Wrap each dataset split in a DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fbf82a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lightning.ai/docs/pytorch/latest/data/datamodule.html\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Note - you must have torchvision installed for this example\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./\"):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\":\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\":\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=32)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=32)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=32)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.mnist_predict, batch_size=32)\n",
    "\n",
    "data_module = MNISTDataModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb11905",
   "metadata": {},
   "source": [
    "# model.py\n",
    "specifies the neural network architecture, the loss function and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a969db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "class LightningMNISTClassifier(pl.LightningModule):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # mnist images are (1, 28, 28) (channels, width, height) \n",
    "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
    "    self.layer_2 = torch.nn.Linear(128, 256)\n",
    "    self.layer_3 = torch.nn.Linear(256, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "      batch_size, channels, width, height = x.size()\n",
    "\n",
    "      # (b, 1, 28, 28) -> (b, 1*28*28)\n",
    "      x = x.view(batch_size, -1)\n",
    "\n",
    "      # layer 1 (b, 1*28*28) -> (b, 128)\n",
    "      x = self.layer_1(x)\n",
    "      x = torch.relu(x)\n",
    "\n",
    "      # layer 2 (b, 128) -> (b, 256)\n",
    "      x = self.layer_2(x)\n",
    "      x = torch.relu(x)\n",
    "\n",
    "      # layer 3 (b, 256) -> (b, 10)\n",
    "      x = self.layer_3(x)\n",
    "\n",
    "      # probability distribution over labels\n",
    "      x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "      return x\n",
    "\n",
    "  def cross_entropy_loss(self, logits, labels):\n",
    "    return F.nll_loss(logits, labels)\n",
    "\n",
    "  def training_step(self, train_batch, batch_idx):\n",
    "      x, y = train_batch\n",
    "      logits = self.forward(x)\n",
    "      loss = self.cross_entropy_loss(logits, y)\n",
    "      self.log('train_loss', loss)\n",
    "      return loss\n",
    "\n",
    "\n",
    "  def validation_step(self, val_batch, batch_idx):\n",
    "      x, y = val_batch\n",
    "      logits = self.forward(x)\n",
    "      loss = self.cross_entropy_loss(logits, y)\n",
    "      self.log('val_loss', loss)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "model = LightningMNISTClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0b9be",
   "metadata": {},
   "source": [
    "# train.py\n",
    "train models and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134017a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Save the model with help from utils.py\n",
    "utils.save_model(model=model1,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"Scratch.pth\")\n",
    "utils.save_model(model=model2,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"ResNet-18.pth\")\n",
    "utils.save_model(model=model3,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"VGG-16.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59998050",
   "metadata": {},
   "source": [
    "# evaluate.py\n",
    "contains the main loop for evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1cb63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d731e7f",
   "metadata": {},
   "source": [
    "# search_hyperparams.py\n",
    "hyper parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96388e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed44f43e",
   "metadata": {},
   "source": [
    "# synthesize_results.py\n",
    "An author synthesizes study data by combining the results together to enable comparison and to allow others to draw further conclusions from them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba26f56",
   "metadata": {},
   "source": [
    "# evaluate.py\n",
    "contains the main loop for evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b82219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c28a1e5",
   "metadata": {},
   "source": [
    "# utils.py\n",
    "utility functions for handling hyperparams/logging/storing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81012979",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ce356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
