{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4d8b89a-750f-4c30-b4eb-119ff4070c05",
   "metadata": {},
   "source": [
    "# Data preparation (customDataset.py)\n",
    "\n",
    "https://www.learnpytorch.io/04_pytorch_custom_datasets/#what-is-a-custom-dataset\n",
    "https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb\n",
    "\n",
    "Build custom dataset\n",
    "\n",
    "- Organize into folders\n",
    "- Turn into tensors\n",
    "\n",
    "Image classification format contains separate classes of images in seperate directories titled with a particular class name.\n",
    "\n",
    "The format we aim for:\n",
    "\n",
    "LBST/ <- overall dataset folder\n",
    "    train/ <- training images\n",
    "        Approved/ <- class name as folder name\n",
    "            Parcel-1\n",
    "                image01.jpeg\n",
    "                image02.jpeg\n",
    "                ...\n",
    "            Parcel-2\n",
    "                image-05.jpeg\n",
    "                image-06.jpeg\n",
    "                ...\n",
    "        Not-Approved/\n",
    "            Parcel-3\n",
    "                image07.jpeg\n",
    "                image08.jpeg\n",
    "                ...\n",
    "            Parcel-4\n",
    "                image-09.jpeg\n",
    "                image-10.jpeg\n",
    "                ...\n",
    "    test/ <- testing images\n",
    "        Approved/ <- class name as folder name\n",
    "            Parcel-5\n",
    "                image11.jpeg\n",
    "                image12.jpeg\n",
    "                ...\n",
    "            Parcel-6\n",
    "                image-13.jpeg\n",
    "                image-14.jpeg\n",
    "                ...\n",
    "        Not-Approved/\n",
    "            Parcel-7\n",
    "                image15.jpeg\n",
    "                image16.jpeg\n",
    "                ...\n",
    "            Parcel-8\n",
    "                image-17.jpeg\n",
    "                image-18.jpeg\n",
    "                ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429fba8d-034c-4f37-b580-0b1699bfdc15",
   "metadata": {},
   "source": [
    "## Create csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dffa8aa4-e782-4cfa-9810-844b331a29eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'Approved', 'NonApproved', 'test_set.csv', 'train_set.csv', 'valid_set.csv', 'xlbst.csv']\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "# Hyperparameters\n",
    "in_channel = 3\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "random_seed = 1\n",
    "\n",
    "set_all_seeds(random_seed)\n",
    "\n",
    "# Find folder paths\n",
    "\n",
    "base_path = r\"C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/\"\n",
    "target_dirs = os.listdir(base_path)\n",
    "print(target_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "722ceadd-f8f8-4a72-940c-c193cbce987c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name  label parcel_id\n",
      "0   22-0223360_F796528C3F8CECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "1   22-0223360_F796528C3F8DECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "2   22-0223360_F796528C3F8EECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "3   22-0223360_F796528C40F1ECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "4   22-0223360_F796528C40F2ECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "..                                               ...    ...       ...\n",
      "35  22-0225160_F78EFF88701BA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "36  22-0225160_F78EFF88701CA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "37  22-0225160_F78EFF88701DA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "38  22-0225160_F78EFF88701EA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "39  22-0225160_F78EFF88701FA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "\n",
      "[80 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create 2 separate df's: one for Approved images, one for NonApproved, containing file_name and label\n",
    "\n",
    "# Assign label = 0 to Approved images\n",
    "approved = pd.DataFrame(data = os.listdir(os.path.join(base_path, target_dirs[1])), columns = ['file_name'])\n",
    "approved = approved.assign(label = 0)\n",
    "\n",
    "# Assign label = 1 to NonApproved images\n",
    "nonapproved = pd.DataFrame(data = os.listdir(os.path.join(base_path, target_dirs[2])), columns = ['file_name'])\n",
    "nonapproved = nonapproved.assign(label = 1)\n",
    "\n",
    "# Merge into 1 df\n",
    "df = pd.concat([approved, nonapproved])\n",
    "\n",
    "# Add parcel_id column containing character 3-10 from file_name column\n",
    "df['parcel_id'] = df['file_name'].str[3:10]\n",
    "print(df)\n",
    "\n",
    "# Write .csv\n",
    "df.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/xlbst.csv', sep = ',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16d25fe1-7635-4558-b8f7-4ca90aa803f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name  label parcel_id\n",
      "0   22-0223605_F78EFF88702EA742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "1   22-0223605_F78EFF887030A742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "2   22-0223605_F78EFF88703AA742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "28  22-0225160_F78EFF887012A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "29  22-0225160_F78EFF887013A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "30  22-0225160_F78EFF887014A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "31  22-0225160_F78EFF887015A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "32  22-0225160_F78EFF887016A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "33  22-0225160_F78EFF887017A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "34  22-0225160_F78EFF88701AA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "35  22-0225160_F78EFF88701BA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "36  22-0225160_F78EFF88701CA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "37  22-0225160_F78EFF88701DA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "38  22-0225160_F78EFF88701EA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "39  22-0225160_F78EFF88701FA742E0530EEE260AEFC6.jpeg      1   0225160\n"
     ]
    }
   ],
   "source": [
    "# Split group by\n",
    "\n",
    "set_all_seeds(random_seed)\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "\n",
    "splitter = GroupShuffleSplit(test_size = .20, n_splits = 1, random_state = random_seed) # n_splits saved 2 splits of train, test and valid: \n",
    "split = splitter.split(df, groups = df.parcel_id)\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_set = df.iloc[train_inds]\n",
    "test_set = df.iloc[test_inds]\n",
    "\n",
    "print(test_set.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e2606db-24d6-429d-874e-556822f8beeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name  label parcel_id\n",
      "0   22-0223360_F796528C3F8CECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "1   22-0223360_F796528C3F8DECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "2   22-0223360_F796528C3F8EECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "3   22-0223360_F796528C40F1ECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "4   22-0223360_F796528C40F2ECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "5   22-0223360_F796528C40F3ECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "6   22-0223360_F796528C4169ECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "7   22-0223360_F796528C416AECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "8   22-0223360_F796528C416BECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "9   22-0223402_F796528C48EDECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "10  22-0223402_F796528C48EEECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "11  22-0223402_F796528C48EFECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "12  22-0223402_F796528C48F0ECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "13  22-0223402_F796528C4963ECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "14  22-0223402_F796528C4964ECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "15  22-0223402_F796528C4986ECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "16  22-0223402_F796528C4988ECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "17  22-0223402_F796528C49AFECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "18  22-0223402_F796528C49B6ECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "19  22-0223803_F796528C438DECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "20  22-0223803_F796528C438EECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "21  22-0223803_F796528C438FECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "22  22-0223803_F796528C4390ECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "23  22-0223803_F796528C44E7ECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "24  22-0223803_F796528C44E8ECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "25  22-0223803_F796528C44E9ECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "26  22-0223803_F796528C44F6ECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "27  22-0223803_F796528C44F8ECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "28  22-0223803_F796528C44FAECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "29  22-0223941_F796528C3DBBECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "30  22-0223941_F796528C3DBDECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "31  22-0223941_F796528C3DBEECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "32  22-0223941_F796528C3E16ECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "33  22-0223941_F796528C3E17ECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "34  22-0223941_F796528C3E18ECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "35  22-0223941_F796528C3E19ECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "36  22-0223941_F796528C3E1AECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "37  22-0223990_F796528C400BECCDE0530EEE260A12DE.jpeg      0   0223990\n",
      "38  22-0223990_F796528C400CECCDE0530EEE260A12DE.jpeg      0   0223990\n",
      "39  22-0223990_F796528C400DECCDE0530EEE260A12DE.jpeg      0   0223990\n",
      "3   22-0223665_F78CE02C2FD76471E0530EEE260A2945.jpeg      1   0223665\n",
      "4   22-0223665_F78CE02C2FD86471E0530EEE260A2945.jpeg      1   0223665\n",
      "5   22-0223665_F78CE02C2FDC6471E0530EEE260A2945.jpeg      1   0223665\n",
      "6   22-0223665_F78CE02C2FDD6471E0530EEE260A2945.jpeg      1   0223665\n",
      "7   22-0223665_F78EFF886EA3A742E0530EEE260AEFC6.jpeg      1   0223665\n",
      "8   22-0223665_F78EFF886EA4A742E0530EEE260AEFC6.jpeg      1   0223665\n",
      "9   22-0223665_F78EFF886EA8A742E0530EEE260AEFC6.jpeg      1   0223665\n",
      "10  22-0223665_F78EFF886EA9A742E0530EEE260AEFC6.jpeg      1   0223665\n",
      "11  22-0224185_F78EFF887124A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "12  22-0224185_F78EFF887125A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "13  22-0224185_F78EFF887126A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "14  22-0224185_F78EFF887127A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "15  22-0224185_F78EFF887128A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "16  22-0224185_F78EFF887129A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "17  22-0224185_F78EFF88712AA742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "18  22-0224185_F78EFF887130A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "19  22-0224185_F78EFF887131A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "20  22-0224185_F78EFF887132A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "21  22-0224185_F78EFF887133A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "22  22-0224185_F78EFF887134A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "23  22-0224626_F78EFF887058A742E0530EEE260AEFC6.jpeg      1   0224626\n",
      "24  22-0224626_F78EFF88705FA742E0530EEE260AEFC6.jpeg      1   0224626\n",
      "25  22-0224626_F78EFF887061A742E0530EEE260AEFC6.jpeg      1   0224626\n",
      "26  22-0224626_F78EFF8870C7A742E0530EEE260AEFC6.jpeg      1   0224626\n",
      "27  22-0224626_F78EFF8870C8A742E0530EEE260AEFC6.jpeg      1   0224626\n",
      "                                           file_name  label parcel_id\n",
      "0   22-0223360_F796528C3F8CECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "1   22-0223360_F796528C3F8DECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "2   22-0223360_F796528C3F8EECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "3   22-0223360_F796528C40F1ECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "4   22-0223360_F796528C40F2ECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "5   22-0223360_F796528C40F3ECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "6   22-0223360_F796528C4169ECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "7   22-0223360_F796528C416AECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "8   22-0223360_F796528C416BECCDE0530EEE260A12DE.jpeg      0   0223360\n",
      "9   22-0223402_F796528C48EDECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "10  22-0223402_F796528C48EEECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "11  22-0223402_F796528C48EFECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "12  22-0223402_F796528C48F0ECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "13  22-0223402_F796528C4963ECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "14  22-0223402_F796528C4964ECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "15  22-0223402_F796528C4986ECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "16  22-0223402_F796528C4988ECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "17  22-0223402_F796528C49AFECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "18  22-0223402_F796528C49B6ECCDE0530EEE260A12DE.jpeg      0   0223402\n",
      "19  22-0223803_F796528C438DECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "20  22-0223803_F796528C438EECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "21  22-0223803_F796528C438FECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "22  22-0223803_F796528C4390ECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "23  22-0223803_F796528C44E7ECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "24  22-0223803_F796528C44E8ECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "25  22-0223803_F796528C44E9ECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "26  22-0223803_F796528C44F6ECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "27  22-0223803_F796528C44F8ECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "28  22-0223803_F796528C44FAECCDE0530EEE260A12DE.jpeg      0   0223803\n",
      "29  22-0223941_F796528C3DBBECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "30  22-0223941_F796528C3DBDECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "31  22-0223941_F796528C3DBEECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "32  22-0223941_F796528C3E16ECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "33  22-0223941_F796528C3E17ECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "34  22-0223941_F796528C3E18ECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "35  22-0223941_F796528C3E19ECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "36  22-0223941_F796528C3E1AECCDE0530EEE260A12DE.jpeg      0   0223941\n",
      "37  22-0223990_F796528C400BECCDE0530EEE260A12DE.jpeg      0   0223990\n",
      "38  22-0223990_F796528C400CECCDE0530EEE260A12DE.jpeg      0   0223990\n",
      "39  22-0223990_F796528C400DECCDE0530EEE260A12DE.jpeg      0   0223990\n",
      "11  22-0224185_F78EFF887124A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "12  22-0224185_F78EFF887125A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "13  22-0224185_F78EFF887126A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "14  22-0224185_F78EFF887127A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "15  22-0224185_F78EFF887128A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "16  22-0224185_F78EFF887129A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "17  22-0224185_F78EFF88712AA742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "18  22-0224185_F78EFF887130A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "19  22-0224185_F78EFF887131A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "20  22-0224185_F78EFF887132A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "21  22-0224185_F78EFF887133A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "22  22-0224185_F78EFF887134A742E0530EEE260AEFC6.jpeg      1   0224185\n",
      "                                           file_name  label parcel_id\n",
      "3   22-0223665_F78CE02C2FD76471E0530EEE260A2945.jpeg      1   0223665\n",
      "4   22-0223665_F78CE02C2FD86471E0530EEE260A2945.jpeg      1   0223665\n",
      "5   22-0223665_F78CE02C2FDC6471E0530EEE260A2945.jpeg      1   0223665\n",
      "6   22-0223665_F78CE02C2FDD6471E0530EEE260A2945.jpeg      1   0223665\n",
      "7   22-0223665_F78EFF886EA3A742E0530EEE260AEFC6.jpeg      1   0223665\n",
      "8   22-0223665_F78EFF886EA4A742E0530EEE260AEFC6.jpeg      1   0223665\n",
      "9   22-0223665_F78EFF886EA8A742E0530EEE260AEFC6.jpeg      1   0223665\n",
      "10  22-0223665_F78EFF886EA9A742E0530EEE260AEFC6.jpeg      1   0223665\n",
      "23  22-0224626_F78EFF887058A742E0530EEE260AEFC6.jpeg      1   0224626\n",
      "24  22-0224626_F78EFF88705FA742E0530EEE260AEFC6.jpeg      1   0224626\n",
      "25  22-0224626_F78EFF887061A742E0530EEE260AEFC6.jpeg      1   0224626\n",
      "26  22-0224626_F78EFF8870C7A742E0530EEE260AEFC6.jpeg      1   0224626\n",
      "27  22-0224626_F78EFF8870C8A742E0530EEE260AEFC6.jpeg      1   0224626\n"
     ]
    }
   ],
   "source": [
    "# Split train into 80/20 train/valid\n",
    "\n",
    "train_set2 = train_set\n",
    "splitter2 = GroupShuffleSplit(test_size = .2, n_splits = 1, random_state = random_seed)\n",
    "split2 = splitter2.split(train_set2, groups = train_set2.parcel_id)\n",
    "train_inds2, valid_inds = next(split2)\n",
    "\n",
    "train_set2 = train_set.iloc[train_inds2]\n",
    "valid_set = train_set.iloc[valid_inds]\n",
    "\n",
    "# Print train, valid, test\n",
    "print(train_set.to_string())\n",
    "print(train_set2)\n",
    "print(valid_set)\n",
    "\n",
    "# Save to csv\n",
    "\n",
    "train_set2.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/train_set.csv', sep = ',', encoding='utf-8', index=False)\n",
    "valid_set.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/valid_set.csv', sep = ',', encoding='utf-8', index=False)\n",
    "test_set.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/test_set.csv', sep = ',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bdaf04-6c57-48d1-83bc-0414dfd5098f",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3a3465-c230-4d97-b333-06c02aa7a099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "<customDataset.LBSTDataset object at 0x000001E6B8894C40>\n"
     ]
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=ZoZHd0Zm3RY&ab_channel=AladdinPersson\n",
    "\n",
    "set_all_seeds(random_seed)\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "\n",
    "from customDataset import LBSTDataset\n",
    "\n",
    "\n",
    "\n",
    "# Load data: From images to tensor\n",
    "\n",
    "train_set = LBSTDataset(csv_file = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/train_set.csv', \n",
    "                      root_dir = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/All',\n",
    "                      transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "\n",
    "test_set = LBSTDataset(csv_file = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/test_set.csv', \n",
    "                      root_dir = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/All',\n",
    "                      transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "\n",
    "image, label = train_set[0]\n",
    "print(type(image))\n",
    "len(train_set)\n",
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aefddb9f-a164-4d52-9d8f-289feeb97fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of tensor for 50th image in train dataset:  torch.Size([3, 224, 224])\n",
      "The label for 50th image in train dataset:  tensor(1)\n",
      "print 1st batch of image tensor: torch.Size([32, 3, 224, 224])\n",
      "print batch of corresponding labels: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "set_all_seeds(random_seed)\n",
    "\n",
    "\n",
    "# Split data properly without information leak\n",
    "\n",
    "#train_set, test_set = torch.utils.data.random_split(dataset, [60,20])\n",
    "\n",
    "print('The shape of tensor for 50th image in train dataset: ',train_set[49][0].shape)\n",
    "print('The label for 50th image in train dataset: ',train_set[49][1])\n",
    "\n",
    "# Traind and test loader\n",
    "\n",
    "train_loader = DataLoader(dataset = train_set,\n",
    "                          batch_size = batch_size,\n",
    "                          shuffle = True)\n",
    "\n",
    "# print batch of image tensor\n",
    "print('print 1st batch of image tensor:', next(iter(train_loader))[0].shape)\n",
    "print('print batch of corresponding labels:', next(iter(train_loader))[1].shape)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_set,\n",
    "                         batch_size = batch_size,\n",
    "                         shuffle = True)\n",
    "\n",
    "# Model\n",
    "\n",
    "set_all_seeds(random_seed)\n",
    "\n",
    "model = torchvision.models.googlenet(weights='GoogLeNet_Weights.DEFAULT')\n",
    "model.to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376b91c5-ed4c-4a8a-a719-01631bc0766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at epoch 0 is 6.375879287719727\n",
      "Checking accuracy on training set\n",
      "Got 34 / 65 with accuracy 52.307692307692314\n",
      "Checking accuracy on test set\n",
      "Got 0 / 15 with accuracy 0.0\n"
     ]
    }
   ],
   "source": [
    "# Train network\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    \n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "        \n",
    "\n",
    "        #forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        #backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        #gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Cost at epoch {epoch} is {sum(losses)/len(losses)}')\n",
    "\n",
    "# Check acc on training\n",
    "\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "            \n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "            num_correct += (predictions == y).sum()\n",
    "            num_samples += predictions.size(0)\n",
    "            \n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100}')\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "print(\"Checking accuracy on training set\")\n",
    "check_accuracy(train_loader, model)\n",
    "\n",
    "print(\"Checking accuracy on test set\")\n",
    "check_accuracy(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c6f5b-bba8-40f8-9fb7-74477a9ae88e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56d272e-9538-45dc-8119-3ae440978ccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc61b1-209c-486f-b879-420fad403557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e71e86-1e0a-4c7d-bbaf-6d05c6789f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68726414-4cd2-4875-80b5-79bd34d17e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f917a7-2147-4233-a84e-552cf66655b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4cfbc8-cc06-4a0e-90e6-f0d0f904924e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c4db5f-347e-4292-93f8-2d1b22b7afbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c66518f-e1f3-4167-896f-e9a566e68684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7300595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92fedcb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kaspe'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f00b6d",
   "metadata": {},
   "source": [
    "# data_loader.py\n",
    "specifies how the data should be fed to the network: LightningDataModule\n",
    "\n",
    "In short, data preparation has 4 steps:\n",
    "\n",
    "- Download images\n",
    "- Image transforms (these are highly subjective).\n",
    "- Generate training, validation and test dataset splits.\n",
    "- Wrap each dataset split in a DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fbf82a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://lightning.ai/docs/pytorch/latest/data/datamodule.html\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Note - you must have torchvision installed for this example\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class MNISTDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./\"):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        MNIST(self.data_dir, train=True, download=True)\n",
    "        MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\":\n",
    "            mnist_full = MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = random_split(mnist_full, [55000, 5000])\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\":\n",
    "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            self.mnist_predict = MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=32)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=32)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=32)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.mnist_predict, batch_size=32)\n",
    "\n",
    "data_module = MNISTDataModule()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb11905",
   "metadata": {},
   "source": [
    "# model.py\n",
    "specifies the neural network architecture, the loss function and evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a969db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.nn import functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "\n",
    "class LightningMNISTClassifier(pl.LightningModule):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    # mnist images are (1, 28, 28) (channels, width, height) \n",
    "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
    "    self.layer_2 = torch.nn.Linear(128, 256)\n",
    "    self.layer_3 = torch.nn.Linear(256, 10)\n",
    "\n",
    "  def forward(self, x):\n",
    "      batch_size, channels, width, height = x.size()\n",
    "\n",
    "      # (b, 1, 28, 28) -> (b, 1*28*28)\n",
    "      x = x.view(batch_size, -1)\n",
    "\n",
    "      # layer 1 (b, 1*28*28) -> (b, 128)\n",
    "      x = self.layer_1(x)\n",
    "      x = torch.relu(x)\n",
    "\n",
    "      # layer 2 (b, 128) -> (b, 256)\n",
    "      x = self.layer_2(x)\n",
    "      x = torch.relu(x)\n",
    "\n",
    "      # layer 3 (b, 256) -> (b, 10)\n",
    "      x = self.layer_3(x)\n",
    "\n",
    "      # probability distribution over labels\n",
    "      x = torch.log_softmax(x, dim=1)\n",
    "\n",
    "      return x\n",
    "\n",
    "  def cross_entropy_loss(self, logits, labels):\n",
    "    return F.nll_loss(logits, labels)\n",
    "\n",
    "  def training_step(self, train_batch, batch_idx):\n",
    "      x, y = train_batch\n",
    "      logits = self.forward(x)\n",
    "      loss = self.cross_entropy_loss(logits, y)\n",
    "      self.log('train_loss', loss)\n",
    "      return loss\n",
    "\n",
    "\n",
    "  def validation_step(self, val_batch, batch_idx):\n",
    "      x, y = val_batch\n",
    "      logits = self.forward(x)\n",
    "      loss = self.cross_entropy_loss(logits, y)\n",
    "      self.log('val_loss', loss)\n",
    "\n",
    "  def configure_optimizers(self):\n",
    "    optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    return optimizer\n",
    "\n",
    "\n",
    "model = LightningMNISTClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0b9be",
   "metadata": {},
   "source": [
    "# train.py\n",
    "train models and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134017a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "trainer.fit(model, data_module)\n",
    "\n",
    "# Save the model with help from utils.py\n",
    "utils.save_model(model=model1,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"Scratch.pth\")\n",
    "utils.save_model(model=model2,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"ResNet-18.pth\")\n",
    "utils.save_model(model=model3,\n",
    "                 target_dir=\"models\",\n",
    "                 model_name=\"VGG-16.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59998050",
   "metadata": {},
   "source": [
    "# evaluate.py\n",
    "contains the main loop for evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d1cb63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d731e7f",
   "metadata": {},
   "source": [
    "# search_hyperparams.py\n",
    "hyper parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96388e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed44f43e",
   "metadata": {},
   "source": [
    "# synthesize_results.py\n",
    "An author synthesizes study data by combining the results together to enable comparison and to allow others to draw further conclusions from them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba26f56",
   "metadata": {},
   "source": [
    "# evaluate.py\n",
    "contains the main loop for evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b82219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c28a1e5",
   "metadata": {},
   "source": [
    "# utils.py\n",
    "utility functions for handling hyperparams/logging/storing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81012979",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.005\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ce356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Attachments",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
