{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "477a43d9-35e5-4476-8de1-e1eba43ce4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Value after * must be an iterable, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m     model \u001b[38;5;241m=\u001b[39m models\u001b[38;5;241m.\u001b[39mvgg16(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVGG16_Weights.DEFAULT\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m     logger \u001b[38;5;241m=\u001b[39m TensorBoardLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtb_logs\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_model\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# tb_logs is the folder, name is the name of the experiment/model\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mNN\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfigpretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mINPUT_SHAPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfigpretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNUM_CLASSES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfigpretrained\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# .to(device)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m     dm \u001b[38;5;241m=\u001b[39m CustomDataModulepretrained(\n\u001b[0;32m     30\u001b[0m         data_dir\u001b[38;5;241m=\u001b[39mconfigpretrained\u001b[38;5;241m.\u001b[39mDATA_DIR,\n\u001b[0;32m     31\u001b[0m         train_csv\u001b[38;5;241m=\u001b[39mconfigpretrained\u001b[38;5;241m.\u001b[39mTRAIN_CSV_1,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m         num_workers\u001b[38;5;241m=\u001b[39mconfigpretrained\u001b[38;5;241m.\u001b[39mNUM_WORKERS,\n\u001b[0;32m     36\u001b[0m     )\n\u001b[0;32m     37\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[0;32m     38\u001b[0m         logger\u001b[38;5;241m=\u001b[39mlogger, \u001b[38;5;66;03m# PyTorch lightning will automatically know what we are logging by looking at our model.py logs\u001b[39;00m\n\u001b[0;32m     39\u001b[0m         accelerator\u001b[38;5;241m=\u001b[39mconfigpretrained\u001b[38;5;241m.\u001b[39mACCELERATOR,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m#        callbacks=[MyPrintingCallback(), EarlyStopping(monitor=\"val_loss\")],\u001b[39;00m\n\u001b[0;32m     45\u001b[0m     )  \u001b[38;5;66;03m# deterministic ensures random seed reproducibility\u001b[39;00m\n",
      "File \u001b[1;32m~\\LBST\\modelpretrained.py:29\u001b[0m, in \u001b[0;36mNN.__init__\u001b[1;34m(self, model, input_shape, learning_rate, num_classes)\u001b[0m\n\u001b[0;32m     26\u001b[0m     param\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# num_filters = model.fc.in_features # returns the size of the output tensor going into the Linear layer from the conv block.\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m n_sizes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_conv_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(n_sizes, num_classes)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n",
      "File \u001b[1;32m~\\LBST\\modelpretrained.py:39\u001b[0m, in \u001b[0;36mNN._get_conv_output\u001b[1;34m(self, shape)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_conv_output\u001b[39m(\u001b[38;5;28mself\u001b[39m, shape):\n\u001b[0;32m     38\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 39\u001b[0m     tmp_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mVariable(torch\u001b[38;5;241m.\u001b[39mrand(batch_size, \u001b[38;5;241m*\u001b[39mshape))\n\u001b[0;32m     42\u001b[0m     output_feat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_features(tmp_input) \n\u001b[0;32m     43\u001b[0m     n_size \u001b[38;5;241m=\u001b[39m output_feat\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Value after * must be an iterable, not int"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "from modelpretrained import NN\n",
    "from customDataModulepretrained import CustomDataModule\n",
    "import configpretrained\n",
    "from callbacks import MyPrintingCallback, EarlyStopping\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch import seed_everything\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "seed_everything(\n",
    "    42, workers=True\n",
    ")  # By setting workers=True in seed_everything(), Lightning derives unique seeds across all dataloader workers and processes for torch, numpy and stdlib random number generators. When turned on, it ensures that e.g. data augmentations are not repeated across workers.\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    model = models.vgg16(weights='VGG16_Weights.DEFAULT')\n",
    "    \n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\") # tb_logs is the folder, name is the name of the experiment/model\n",
    "    model = NN(\n",
    "        model=model,\n",
    "        input_shape=configpretrained.INPUT_SHAPE,\n",
    "        num_classes=configpretrained.NUM_CLASSES,\n",
    "        learning_rate=configpretrained.LEARNING_RATE,\n",
    "    )  # .to(device)\n",
    "    dm = CustomDataModulepretrained(\n",
    "        data_dir=configpretrained.DATA_DIR,\n",
    "        train_csv=configpretrained.TRAIN_CSV_1,\n",
    "        val_csv=configpretrained.VAL_CSV_1,\n",
    "        test_csv=configpretrained.TEST_CSV,\n",
    "        batch_size=configpretrained.BATCH_SIZE,\n",
    "        num_workers=configpretrained.NUM_WORKERS,\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger, # PyTorch lightning will automatically know what we are logging by looking at our model.py logs\n",
    "        accelerator=configpretrained.ACCELERATOR,\n",
    "        devices=configpretrained.DEVICES,\n",
    "        min_epochs=configpretrained.MIN_EPOCHS,\n",
    "        max_epochs=configpretrained.MAX_EPOCHS,\n",
    "        deterministic=configpretrained.DETERMINISTIC#,\n",
    "#        callbacks=[MyPrintingCallback(), EarlyStopping(monitor=\"val_loss\")],\n",
    "    )  # deterministic ensures random seed reproducibility\n",
    "\n",
    "    trainer.fit(model, dm)  # it will automatically know which dataloader to use\n",
    "\n",
    "# A general place to start is to set num_workers equal to the number of CPU cores on that machine. You can get the number of CPU cores in python using os.cpu_count(), but note that depending on your batch size, you may overflow RAM memory.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
