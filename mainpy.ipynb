{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e4aec7f-e620-4c07-a35e-1d44d1ea9786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'Approved', 'NonApproved', 'test_set.csv', 'train_set.csv', 'val_set.csv', 'xlbst.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name  label parcel_id\n",
      "0   22-0223605_F78EFF88702EA742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "1   22-0223605_F78EFF887030A742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "2   22-0223605_F78EFF88703AA742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "28  22-0225160_F78EFF887012A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "29  22-0225160_F78EFF887013A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "30  22-0225160_F78EFF887014A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "31  22-0225160_F78EFF887015A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "32  22-0225160_F78EFF887016A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "33  22-0225160_F78EFF887017A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "34  22-0225160_F78EFF88701AA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "35  22-0225160_F78EFF88701BA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "36  22-0225160_F78EFF88701CA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "37  22-0225160_F78EFF88701DA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "38  22-0225160_F78EFF88701EA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "39  22-0225160_F78EFF88701FA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "Test set length:\n",
      "15\n",
      "Train set length:\n",
      "65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | fc1      | Linear             | 7.5 M \n",
      "1 | fc2      | Linear             | 102   \n",
      "2 | loss_fn  | CrossEntropyLoss   | 0     \n",
      "3 | accuracy | MulticlassAccuracy | 0     \n",
      "4 | f1_score | MulticlassF1Score  | 0     \n",
      "------------------------------------------------\n",
      "7.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.5 M     Total params\n",
      "30.106    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaspe\\.conda\\envs\\LBST\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:05<00:00,  2.93s/it, v_num=27]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 111.39it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [00:11<00:00,  5.92s/it, v_num=27]96it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:06<00:00,  3.02s/it, v_num=27, train_loss=9.760, train_accuracy=0.423, train_f1_score=0.423]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 111.41it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:12<00:00,  6.07s/it, v_num=27, train_loss=9.760, train_accuracy=0.423, train_f1_score=0.423]\n",
      "Epoch 2: 100%|██████████| 2/2 [00:06<00:00,  3.01s/it, v_num=27, train_loss=7.070, train_accuracy=0.827, train_f1_score=0.827]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 125.33it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:11<00:00,  5.90s/it, v_num=27, train_loss=7.070, train_accuracy=0.827, train_f1_score=0.827]\n",
      "Epoch 3: 100%|██████████| 2/2 [00:05<00:00,  2.60s/it, v_num=27, train_loss=13.30, train_accuracy=0.442, train_f1_score=0.442]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 111.42it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 2/2 [00:10<00:00,  5.11s/it, v_num=27, train_loss=13.30, train_accuracy=0.442, train_f1_score=0.442]\n",
      "Epoch 4: 100%|██████████| 2/2 [00:06<00:00,  3.08s/it, v_num=27, train_loss=7.860, train_accuracy=0.769, train_f1_score=0.769]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 111.40it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 2/2 [00:12<00:00,  6.08s/it, v_num=27, train_loss=7.860, train_accuracy=0.769, train_f1_score=0.769]\n",
      "Epoch 4: 100%|██████████| 2/2 [00:12<00:00,  6.08s/it, v_num=27, train_loss=9.490, train_accuracy=0.769, train_f1_score=0.769]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2/2 [00:12<00:00,  6.20s/it, v_num=27, train_loss=9.490, train_accuracy=0.769, train_f1_score=0.769]\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 95.49it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     4.65419340133667      </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    4.65419340133667     \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 100.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     4.65419340133667      </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    4.65419340133667     \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch import seed_everything\n",
    "import random\n",
    "\n",
    "import config\n",
    "\n",
    "# Create .csv file\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "## Find folder paths\n",
    "\n",
    "base_path = r\"C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/\"\n",
    "target_dirs = os.listdir(base_path)\n",
    "print(target_dirs)\n",
    "\n",
    "## Create 2 separate df's: one for Approved images, one for NonApproved, containing file_name and label\n",
    "\n",
    "### Assign label = 0 to Approved images\n",
    "approved = pd.DataFrame(data = os.listdir(os.path.join(base_path, target_dirs[1])), columns = ['file_name'])\n",
    "approved = approved.assign(label = 0)\n",
    "\n",
    "### Assign label = 1 to NonApproved images\n",
    "nonapproved = pd.DataFrame(data = os.listdir(os.path.join(base_path, target_dirs[2])), columns = ['file_name'])\n",
    "nonapproved = nonapproved.assign(label = 1)\n",
    "\n",
    "## Merge into 1 df\n",
    "df = pd.concat([approved, nonapproved])\n",
    "\n",
    "## Add parcel_id column containing character 3-10 from file_name column\n",
    "df['parcel_id'] = df['file_name'].str[3:10]\n",
    "\n",
    "## Write .csv\n",
    "df.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/xlbst.csv', sep = ',', encoding='utf-8', index=False)\n",
    "\n",
    "\n",
    "\n",
    "## Split group by\n",
    "\n",
    "set_all_seeds(config.SEED)\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.20, n_splits=2, random_state=config.SEED)\n",
    "split = splitter.split(df, groups=df.parcel_id)\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_set = df.iloc[train_inds]\n",
    "test_set = df.iloc[test_inds]\n",
    "\n",
    "\n",
    "print(test_set.to_string())\n",
    "print('Test set length:',len(test_set), 'Train set length:', len(train_set), sep='\\n')\n",
    "\n",
    "\n",
    "## Split train into 80/20 train/val\n",
    "\n",
    "train_set2 = train_set\n",
    "splitter2 = GroupShuffleSplit(test_size=0.2, n_splits =1, random_state=config.SEED)\n",
    "split2 = splitter2.split(train_set2, groups=train_set2.parcel_id)\n",
    "train_inds2, val_inds = next(split2)\n",
    "\n",
    "train_set2 = train_set.iloc[train_inds2]\n",
    "val_set = train_set.iloc[val_inds]\n",
    "\n",
    "\n",
    "## Save to csv\n",
    "\n",
    "train_set2.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/train_set.csv', sep = ',', encoding='utf-8', index=False)\n",
    "val_set.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/val_set.csv', sep = ',', encoding='utf-8', index=False)\n",
    "test_set.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/test_set.csv', sep = ',', encoding='utf-8', index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Train model\n",
    "\n",
    "from model import NN\n",
    "from dataset import CustomDataModule\n",
    "import config\n",
    "seed_everything(42, workers=True) # By setting workers=True in seed_everything(), Lightning derives unique seeds across all dataloader workers and processes for torch, numpy and stdlib random number generators. When turned on, it ensures that e.g. data augmentations are not repeated across workers.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = NN(input_size=config.INPUT_SIZE, num_classes=config.NUM_CLASSES, learning_rate=config.LEARNING_RATE)#.to(device)\n",
    "    dm = CustomDataModule(data_dir=config.DATA_DIR,\n",
    "                      train_csv=config.TRAIN_CSV,\n",
    "                      val_csv=config.VAL_CSV,\n",
    "                      test_csv=config.VAL_CSV,\n",
    "                      batch_size=config.BATCH_SIZE,\n",
    "                      num_workers=config.NUM_WORKERS)\n",
    "    trainer = pl.Trainer(accelerator=config.ACCELERATOR, devices=config.DEVICES, min_epochs=config.MIN_EPOCHS, max_epochs=config.MAX_EPOCHS, deterministic=config.DETERMINISTIC) # deterministic ensures random seed reproducibility\n",
    "    trainer.fit(model, dm) # it will automatically know which dataloader to use\n",
    "    trainer.validate(model, dm)\n",
    "    trainer.test(model, dm)\n",
    "\n",
    "# A general place to start is to set num_workers equal to the number of CPU cores on that machine. You can get the number of CPU cores in python using os.cpu_count(), but note that depending on your batch size, you may overflow RAM memory.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
