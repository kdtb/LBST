{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e1a4c98-53e8-4060-998d-6e252ee96995",
   "metadata": {},
   "source": [
    "## Check that CustomDataModule works\n",
    "\n",
    "The reason I want to check this is that the augmented images that i logged through tensorboard were completely off. So i wanted to make sure, that the CustomDataModule class works properly. Either it is the dataloaders not working or the images are used wrong in my model.py\n",
    "\n",
    "1. Instantiate the CustomDataModule and its .setup and .train_dataloader to be able to extract tensor batches from it\n",
    "2. Create a data variable containing images as tensors (of shape torch.Size([16, 3, 224, 224]))\n",
    "3. Convert back to actual image using ToPILImage\n",
    "4. Display it with PIL\n",
    "\n",
    "So i manually sample from my Dataset and convert the image back to an actual image, then display it with PIL\n",
    "If it displays an image you recognise then you're not loading images wrong, you might just be using them wrong in the model\n",
    "\n",
    "\n",
    "The code below only works if the dataloaders in customDataModule does not normalize the tensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab8f0d26-68d7-4a66-b642-726f16c55ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "from customDataModule import CustomDataModule\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# Instantiate class\n",
    "\n",
    "dm = CustomDataModule(\n",
    "        data_dir=config.DATA_DIR,\n",
    "        train_csv=config.TRAIN_CSV,\n",
    "        val_csv=config.VAL_CSV,\n",
    "        test_csv=config.VAL_CSV,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "    )\n",
    "\n",
    "## Instantiate objects\n",
    "dm.setup(\"train\") # setup skal instantiates for at kunne lave train_dataloader\n",
    "dm.train_dataloader()\n",
    "\n",
    "# Take out the first batch of tensor images\n",
    "train_dataloader = dm.train_dataloader()\n",
    "train_dataloader_iterator = iter(train_dataloader) # Use iter() to generate an iterator for the dataloader\n",
    "data = next(train_dataloader_iterator) # use next() function on the iterator object to get 1st batch of data\n",
    "data\n",
    "#data # \"data will now contain the first batch of data from train_dataloader, \n",
    "     # which may include one or multiple images depending on the batch size. If the batch size is 16, data will have the shape [16, C, H, W], where C, H, and W are the dimensions of the image in channel, height, and width, respectively.\"\n",
    "\n",
    "\n",
    "# Convert tensor to PIL image\n",
    "\n",
    "## Get first image from batch\n",
    "image_tensor = data[0][1]   # data should have shape 16: [16, C, H, W]\n",
    "print(image_tensor.shape)\n",
    "\n",
    "## define a transform to convert a tensor to PIL image\n",
    "transform = T.ToPILImage()\n",
    "\n",
    "## convert the tensor to PIL image using above transform\n",
    "image_tensor = transform(image_tensor)\n",
    "\n",
    "## display the PIL image\n",
    "\n",
    "image_tensor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03110d93-61db-4cda-aa0d-b6f3f8fa9199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'Approved', 'NonApproved', 'test_set.csv', 'train_set.csv', 'val_set.csv', 'xlbst.csv']\n",
      "                                           file_name  label parcel_id\n",
      "0   22-0223605_F78EFF88702EA742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "1   22-0223605_F78EFF887030A742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "2   22-0223605_F78EFF88703AA742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "28  22-0225160_F78EFF887012A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "29  22-0225160_F78EFF887013A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "30  22-0225160_F78EFF887014A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "31  22-0225160_F78EFF887015A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "32  22-0225160_F78EFF887016A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "33  22-0225160_F78EFF887017A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "34  22-0225160_F78EFF88701AA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "35  22-0225160_F78EFF88701BA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "36  22-0225160_F78EFF88701CA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "37  22-0225160_F78EFF88701DA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "38  22-0225160_F78EFF88701EA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "39  22-0225160_F78EFF88701FA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "Test set length:\n",
      "15\n",
      "Train set length:\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "from createCSV import createCSV\n",
    "import config\n",
    "csv = createCSV(\n",
    "        base_dir = config.BASE_DIR,\n",
    "        all_csv = config.ALL_CSV,\n",
    "        train_csv = config.TRAIN_CSV,\n",
    "        val_csv = config.VAL_CSV,\n",
    "        test_csv = config.TEST_CSV,\n",
    "        label_column = config.LABEL_COLUMN,\n",
    "        test_size = config.TEST_SIZE,\n",
    "        seed = config.SEED)\n",
    "csv.set_all_seeds()\n",
    "csv.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd844352-810f-4eb3-b7fa-8421e2eea264",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name     | Type               | Params\n",
      "------------------------------------------------\n",
      "0 | fc1      | Linear             | 7.5 M \n",
      "1 | fc2      | Linear             | 102   \n",
      "2 | loss_fn  | CrossEntropyLoss   | 0     \n",
      "3 | accuracy | MulticlassAccuracy | 0     \n",
      "4 | f1_score | MulticlassF1Score  | 0     \n",
      "------------------------------------------------\n",
      "7.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.5 M     Total params\n",
      "30.106    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                            \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaspe\\.conda\\envs\\LBST\\lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:280: PossibleUserWarning: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train!\n",
      "Epoch 0: 100%|██████████| 4/4 [00:05<00:00,  1.49s/it, v_num=8]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 4/4 [00:11<00:00,  2.79s/it, v_num=8].24it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 4/4 [00:06<00:00,  1.52s/it, v_num=8, train_loss=7.440, train_accuracy=0.577, train_f1_score=0.577]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 4/4 [00:11<00:00,  2.81s/it, v_num=8, train_loss=7.440, train_accuracy=0.577, train_f1_score=0.577]\n",
      "Epoch 2: 100%|██████████| 4/4 [00:06<00:00,  1.59s/it, v_num=8, train_loss=1.850, train_accuracy=0.769, train_f1_score=0.769]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 4/4 [00:11<00:00,  2.90s/it, v_num=8, train_loss=1.850, train_accuracy=0.769, train_f1_score=0.769]\n",
      "Epoch 3: 100%|██████████| 4/4 [00:06<00:00,  1.56s/it, v_num=8, train_loss=7.100, train_accuracy=0.788, train_f1_score=0.788]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 4/4 [00:11<00:00,  2.86s/it, v_num=8, train_loss=7.100, train_accuracy=0.788, train_f1_score=0.788]\n",
      "Epoch 3: 100%|██████████| 4/4 [00:11<00:00,  2.86s/it, v_num=8, train_loss=2.650, train_accuracy=0.769, train_f1_score=0.769]Training is done!\n",
      "Epoch 3: 100%|██████████| 4/4 [00:11<00:00,  2.91s/it, v_num=8, train_loss=2.650, train_accuracy=0.769, train_f1_score=0.769]\n",
      "Validation DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 91.17it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.0658938884735107     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.0658938884735107    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 1/1 [00:00<00:00, 91.39it/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.0658938884735107     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.0658938884735107    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "from model import NN\n",
    "from customDataModule import CustomDataModule\n",
    "import config\n",
    "from callbacks import MyPrintingCallback, EarlyStopping\n",
    "import pytorch_lightning as pl\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch import seed_everything\n",
    "\n",
    "seed_everything(\n",
    "    42, workers=True\n",
    ")  # By setting workers=True in seed_everything(), Lightning derives unique seeds across all dataloader workers and processes for torch, numpy and stdlib random number generators. When turned on, it ensures that e.g. data augmentations are not repeated across workers.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger = TensorBoardLogger(\"tb_logs\", name=\"my_model\") # tb_logs is the folder, name is the name of the experiment/model\n",
    "    model = NN(\n",
    "        input_size=config.INPUT_SIZE,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "    )  # .to(device)\n",
    "    dm = CustomDataModule(\n",
    "        data_dir=config.DATA_DIR,\n",
    "        train_csv=config.TRAIN_CSV,\n",
    "        val_csv=config.VAL_CSV,\n",
    "        test_csv=config.VAL_CSV,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "    )\n",
    "    trainer = pl.Trainer(\n",
    "        logger=logger, # PyTorch lightning will automatically know what we are logging by looking at our model.py logs\n",
    "        accelerator=config.ACCELERATOR,\n",
    "        devices=config.DEVICES,\n",
    "        min_epochs=config.MIN_EPOCHS,\n",
    "        max_epochs=config.MAX_EPOCHS,\n",
    "        deterministic=config.DETERMINISTIC,\n",
    "        callbacks=[MyPrintingCallback(), EarlyStopping(monitor=\"val_loss\")],\n",
    "    )  # deterministic ensures random seed reproducibility\n",
    "    trainer.fit(model, dm)  # it will automatically know which dataloader to use\n",
    "    trainer.validate(model, dm)\n",
    "    trainer.test(model, dm)\n",
    "\n",
    "# A general place to start is to set num_workers equal to the number of CPU cores on that machine. You can get the number of CPU cores in python using os.cpu_count(), but note that depending on your batch size, you may overflow RAM memory.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
