{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50907ddb-57fc-421d-b038-660803f6c94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from torchmetrics import Metric\n",
    "def set_all_seeds(seed):\n",
    "    os.environ[\"PL_GLOBAL_SEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "74d63d06-b4b0-4d81-b106-0e07efd742a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "babc847c-e88f-4b58-999d-332c4c5023a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce code length by creating a _common_step function, in order not to have to repeat code in training_step, validation_step and test_step\n",
    "\n",
    "class MyAccuracy(Metric):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.add_state(\"total\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "            self.add_state(\"correct\", default=torch.tensor(0), dist_reduce_fx=\"sum\")\n",
    "        \n",
    "        def update(self, preds, target):\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            assert preds.shape == target.shape\n",
    "            self.correct += torch.sum(preds == target)\n",
    "            self.total += target.numel()\n",
    "            \n",
    "        def compute(self):\n",
    "            return self.correct.float() / self.total.float()\n",
    "\n",
    "class NN(pl.LightningModule): # pl.LightningModule inherits from nn.Module and adds extra functionality\n",
    "    def __init__(self, input_size, num_classes): # In the constructor, you declare all the layers you want to use.\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.my_accuracy = MyAccuracy()\n",
    "        self.f1_score = torchmetrics.F1Score(task=\"multiclass\", num_classes=num_classes)\n",
    "        \n",
    "    def forward(self, x): # Forward function computes output Tensors from input Tensors. In the forward function, you define how your model is going to be run, from input to output. We're accepting only a single input in here, but if you want, feel free to use more\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        accuracy = self.my_accuracy(scores, y)\n",
    "        f1_score = self.f1_score(scores, y)\n",
    "        self.log_dict({'train_loss': loss, 'train_accuracy': accuracy, 'train_f1_score': f1_score},\n",
    "                     on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return {'loss': loss, \"scores\": scores, \"y\": y}\n",
    "    \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, scores, y = self._common_step(batch, batch_idx)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        loss = self.loss_fn(scores, y)\n",
    "        return loss, scores, y\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1) # flattening\n",
    "        scores = self.forward(x)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4b0827-4645-4df4-b06c-5f0794b06a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from customDataset import LBSTDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, train_csv_file, val_csv_file, test_csv_file, batch_size, num_workers):\n",
    "        self.data_dir = data_dir\n",
    "        self.train_csv_file = train_csv_file\n",
    "        self.val_csv_file = val_csv_file\n",
    "        self.test_csv_file = test_csv_file\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "    \n",
    "#    def prepare_data(self): # downloading the data here so we have it to disc\n",
    "#        LBSTDataset(train_csv_file, data_dir, transform=None)\n",
    "#        LBSTDataset(val_csv_file, data_dir, transform=None)\n",
    "#        LBSTDataset(test_csv_file, data_dir, transform=None)\n",
    "        # single gpu\n",
    "\n",
    "    \n",
    "    def setup(stage):\n",
    "        # multiple gpu\n",
    "        self.train_set = LBSTDataset(\n",
    "            csv_file=self.train_csv_file,\n",
    "            root_dir=self.data_dir,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                #transforms.RandomHorizontalFlip(),\n",
    "                #transforms.RandomResizedCrop(224),\n",
    "                transforms.ToTensor()#,\n",
    "                #transforms.Normalize()\n",
    "            ])\n",
    "        self.val_set = LBSTDataset(\n",
    "            csv_file=self.val_csv_file,\n",
    "            root_dir=self.data_dir,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                #transforms.RandomHorizontalFlip(),\n",
    "                #transforms.RandomResizedCrop(224),\n",
    "                transforms.ToTensor()#,\n",
    "                #transforms.Normalize()\n",
    "            ])\n",
    "        self.test_set = LBSTDataset(\n",
    "            csv_file=self.test_csv_file,\n",
    "            root_dir=self.data_dir,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize((224, 224)),\n",
    "                #transforms.RandomHorizontalFlip(),\n",
    "                #transforms.RandomResizedCrop(224),\n",
    "                transforms.ToTensor()#,\n",
    "                #transforms.Normalize()\n",
    "            ])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.train_set,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.val_set,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            dataset=self.test_set,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f6d1d8-1cc5-4e87-bd2e-a4b5c577e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device cuda for GPU if it's available, otherwise run on the CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 3 * 224 * 224\n",
    "num_classes = 2\n",
    "learning_rate = 1e-3\n",
    "batch_size = 32\n",
    "num_epochs = 3\n",
    "random_seed = 1\n",
    "set_all_seeds(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a429a32-b9b2-4772-96d5-9b581a927692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all.csv', 'Approved', 'Danish Challenge', 'new_train_set.csv', 'NonApproved', 'test_set.csv', 'train_set.csv', 'val_set.csv']\n"
     ]
    }
   ],
   "source": [
    "# Find folder paths\n",
    "\n",
    "base_path = r\"C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/\"\n",
    "target_dirs = os.listdir(base_path)\n",
    "print(target_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4ce751e-64e0-4586-b91f-fd9c9af60343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 separate df's: one for Approved images, one for NonApproved, containing file_name and label\n",
    "\n",
    "# Assign label = 0 to Approved images\n",
    "#approved = pd.DataFrame(data = os.listdir(os.path.join(base_path, target_dirs[1])), columns = ['file_name'])\n",
    "#approved = approved.assign(label = 0)\n",
    "\n",
    "# Assign label = 1 to NonApproved images\n",
    "approved = pd.DataFrame(data = os.listdir(os.path.join(base_path, target_dirs[1])), columns = ['file_name'])\n",
    "approved = approved.assign(label = 0)\n",
    "\n",
    "# Merge into 1 df\n",
    "#df = pd.concat([approved, nonapproved])\n",
    "\n",
    "# Add parcel_id column containing character 3-10 from file_name column\n",
    "approved['journal_id'] = approved['file_name'].str[0:10]\n",
    "approved = approved.drop_duplicates(subset = ['journal_id'])\n",
    "\n",
    "# Write .csv\n",
    "approved.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Approved.csv', sep = ',', encoding='utf-8', index=False)\n",
    "approved.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Approved.txt', header=None, index=None, sep=' ', mode='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6e50f125-25c0-4b2c-bd7f-454dcefe8bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           file_name  label parcel_id\n",
      "0   22-0223605_F78EFF88702EA742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "1   22-0223605_F78EFF887030A742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "2   22-0223605_F78EFF88703AA742E0530EEE260AEFC6.jpeg      1   0223605\n",
      "28  22-0225160_F78EFF887012A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "29  22-0225160_F78EFF887013A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "30  22-0225160_F78EFF887014A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "31  22-0225160_F78EFF887015A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "32  22-0225160_F78EFF887016A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "33  22-0225160_F78EFF887017A742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "34  22-0225160_F78EFF88701AA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "35  22-0225160_F78EFF88701BA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "36  22-0225160_F78EFF88701CA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "37  22-0225160_F78EFF88701DA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "38  22-0225160_F78EFF88701EA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "39  22-0225160_F78EFF88701FA742E0530EEE260AEFC6.jpeg      1   0225160\n",
      "Test set length:\n",
      "15\n",
      "Train set length:\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# Split group by\n",
    "\n",
    "set_all_seeds(random_seed)\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "splitter = GroupShuffleSplit(test_size=0.20, n_splits=2, random_state=random_seed)\n",
    "split = splitter.split(df, groups=df.parcel_id)\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_set = df.iloc[train_inds]\n",
    "test_set = df.iloc[test_inds]\n",
    "\n",
    "\n",
    "print(test_set.to_string())\n",
    "print('Test set length:',len(test_set), 'Train set length:', len(train_set), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5ef860cb-52a0-4d22-a6c9-68e95b955b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into 80/20 train/val\n",
    "\n",
    "train_set2 = train_set\n",
    "splitter2 = GroupShuffleSplit(test_size=0.2, n_splits =1, random_state=random_seed)\n",
    "split2 = splitter2.split(train_set2, groups=train_set2.parcel_id)\n",
    "train_inds2, val_inds = next(split2)\n",
    "\n",
    "train_set2 = train_set.iloc[train_inds2]\n",
    "val_set = train_set.iloc[val_inds]\n",
    "\n",
    "# Print train, val, test\n",
    "#print(train_set.to_string())\n",
    "#print(train_set2)\n",
    "#print(val_set)\n",
    "#print(len(train_set), len(train_set2), len(val_set))\n",
    "\n",
    "# Save to csv\n",
    "\n",
    "train_set2.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/train_set.csv', sep = ',', encoding='utf-8', index=False)\n",
    "val_set.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/val_set.csv', sep = ',', encoding='utf-8', index=False)\n",
    "test_set.to_csv(r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/test_set.csv', sep = ',', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "4bbbaf77-cf65-41a9-a3f5-8cc1b4ebccbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<customDataset.LBSTDataset object at 0x000001DB06550730>\n",
      "The shape of tensor for 50th image in train dataset:  torch.Size([3, 224, 224])\n",
      "The label for 50th image in train dataset:  tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# https://www.youtube.com/watch?v=ZoZHd0Zm3RY&ab_channel=AladdinPersson\n",
    "\n",
    "set_all_seeds(random_seed)\n",
    "\n",
    "# Create PyTorch compatible datasets: From images to tensors\n",
    "\n",
    "from customDataset import LBSTDataset\n",
    "train_set = LBSTDataset(csv_file = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/train_set.csv', \n",
    "                      root_dir = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/All',\n",
    "                      transform = transforms.Compose([\n",
    "                                                transforms.Resize((224, 224)),\n",
    "                                                #transforms.RandomHorizontalFlip(),\n",
    "                                                #transforms.RandomResizedCrop(224),\n",
    "                                                transforms.ToTensor()#,\n",
    "                                                #transforms.Normalize()\n",
    "                      ]))\n",
    "\n",
    "val_set = LBSTDataset(csv_file = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/val_set.csv', \n",
    "                      root_dir = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/All',\n",
    "                      transform = transforms.Compose([\n",
    "                                                transforms.Resize((224, 224)),\n",
    "                                                #transforms.RandomHorizontalFlip(),\n",
    "                                                #transforms.RandomResizedCrop(224),\n",
    "                                                transforms.ToTensor()#,\n",
    "                                                #transforms.Normalize()\n",
    "                      ]))\n",
    "\n",
    "test_set = LBSTDataset(csv_file = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/test_set.csv', \n",
    "                      root_dir = r'C:/Users/kaspe/OneDrive - Aarhus Universitet/Skrivebord/BI/4. semester/Data/LBST/Danish Challenge/2023 J#/All',\n",
    "                      transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "]))\n",
    "\n",
    "len(train_set)\n",
    "print(train_set)\n",
    "print('The shape of tensor for 50th image in train dataset: ',train_set[49][0].shape)\n",
    "print('The label for 50th image in train dataset: ',train_set[49][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c205fe32-ad98-45ad-b0de-72f510ddc868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print 1st batch of image tensor: torch.Size([32, 3, 224, 224])\n",
      "print batch of corresponding labels: torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "set_all_seeds(random_seed)\n",
    "\n",
    "# Train, val and test loader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True) # Shuffling is done during the training to make sure we aren’t exposing our model to the same cycle (order) of data in every epoch. \n",
    "\n",
    "val_loader = DataLoader(dataset=val_set,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=False) # You don’t need to shuffle the validation and test datasets, since no training is done, the model is used in model.eval() and thus the order of samples won’t change the results.\n",
    "\n",
    "test_loader = DataLoader(dataset=test_set,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=False)\n",
    "\n",
    "# print batch of image tensor\n",
    "print('print 1st batch of image tensor:', next(iter(train_loader))[0].shape)\n",
    "print('print batch of corresponding labels:', next(iter(train_loader))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "306a1bb2-37ff-40ce-baac-44c053be5cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(input_size=input_size, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8cc6bf1e-e4a1-4bdf-b131-0fddf0c71f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.cpu_count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ef4b4b22-c872-4033-b5e0-5d6d73ff9786",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type               | Params\n",
      "---------------------------------------------------\n",
      "0 | fc1         | Linear             | 7.5 M \n",
      "1 | fc2         | Linear             | 102   \n",
      "2 | loss_fn     | CrossEntropyLoss   | 0     \n",
      "3 | accuracy    | MulticlassAccuracy | 0     \n",
      "4 | my_accuracy | MyAccuracy         | 0     \n",
      "5 | f1_score    | MulticlassF1Score  | 0     \n",
      "---------------------------------------------------\n",
      "7.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.5 M     Total params\n",
      "30.106    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s, v_num=21]            \n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s, v_num=21]25it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:01<00:00,  1.42it/s, v_num=21, train_loss=6.770, train_accuracy=0.231, train_f1_score=0.231]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 2/2 [00:01<00:00,  1.09it/s, v_num=21, train_loss=6.770, train_accuracy=0.231, train_f1_score=0.231]\n",
      "Epoch 2: 100%|██████████| 2/2 [00:01<00:00,  1.44it/s, v_num=21, train_loss=0.642, train_accuracy=0.769, train_f1_score=0.769]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s, v_num=21, train_loss=0.642, train_accuracy=0.769, train_f1_score=0.769]\n",
      "Epoch 2: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s, v_num=21, train_loss=0.641, train_accuracy=0.769, train_f1_score=0.769]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2/2 [00:02<00:00,  1.02s/it, v_num=21, train_loss=0.641, train_accuracy=0.769, train_f1_score=0.769]\n"
     ]
    }
   ],
   "source": [
    "# Train network\n",
    "from lightning.pytorch import Trainer, seed_everything\n",
    "\n",
    "seed_everything(42, workers=True) # By setting workers=True in seed_everything(), Lightning derives unique seeds across all dataloader workers and processes for torch, numpy and stdlib random number generators. When turned on, it ensures that e.g. data augmentations are not repeated across workers.\n",
    "\n",
    "trainer = pl.Trainer(accelerator='auto', min_epochs=1, max_epochs=3, deterministic=True) # deterministic ensures random seed reproducibility\n",
    "# trainer.tune finds optimal hyper parameters, eg batch size and learning rate\n",
    "\n",
    "trainer.fit(model, train_loader, val_loader)\n",
    "trainer.validate(model, val_loader)\n",
    "trainer.test(model, test_loader)\n",
    "\n",
    "# A general place to start is to set num_workers equal to the number of CPU cores on that machine. You can get the number of CPU cores in python using os.cpu_count(), but note that depending on your batch size, you may overflow RAM memory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5f481a-a06e-4d6d-9831-4ab5c9a3202f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1852e048-f989-4adf-b8a9-063ed37a8dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbc029c-c7b7-4f55-b840-68af44817f49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e022e8a-891a-4cb6-b525-aba88d04f8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c5b4c-43cf-42c9-94f2-ad8c1488411a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "class NN(pl.LightningModule): # pl.LightningModule inherits from nn.Module and adds extra functionality\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 50)\n",
    "        self.fc2 = nn.Linear(50, num_classes)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x) # compute scores\n",
    "        loss = F.loss_fn(scores, y) # compute losses\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x) # compute scores\n",
    "        loss = F.loss_fn(scores, y) # compute losses\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x) # compute scores\n",
    "        loss = F.loss_fn(scores, y) # compute losses\n",
    "        self.log('test_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        scores = self.forward(x)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
